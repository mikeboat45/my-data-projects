# -*- coding: utf-8 -*-
"""Retail and Customer Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SuEfoOT1KR81QAP8LOs4Ncm_YhsT6PcT

**Goals:**
1. Understand the types of customers who purchase chips
2. Identify the purchasing behaviour of customers between regions
3. Form a strategy based on the findings to provide a clear recommendation to the Category Manager
"""

#importing important libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df1 = pd.read_excel('/content/drive/MyDrive/Projects/Quantium Data Analytics Job Simulation/QVI_transaction_data.xlsx')
df1.shape

df1.head()

df1.info()

df1['DATE'] = pd.to_datetime(df1['DATE'], unit='D', origin='1899-12-30')

print(df1['DATE'].dtype)

df1.describe(include='object')

import re

# 1. Extract unique words from 'PROD_NAME'
productWords = pd.Series(' '.join(df1['PROD_NAME'].unique()).split()).unique()

# 2. Remove words with digits and special characters
productWords = [word for word in productWords if re.match(r'^[a-zA-Z]+$', word)]

# Now 'productWords' contains a list of unique words from 'PROD_NAME' without digits and special characters.

print(productWords)


from collections import Counter

word_counts = Counter(productWords)  # Count word frequencies

# Sort words by frequency (descending)
sorted_words = word_counts.most_common()

# Print the sorted words and their frequencies
for word, count in sorted_words:
    print(f"{word}: {count}")

import pandas as pd

# 1. Create a boolean mask for salsa products
salsa_mask = df1['PROD_NAME'].str.lower().str.contains('salsa')

# 2. Filter the DataFrame to exclude salsa products
df1 = df1[~salsa_mask]

df1.shape

df1.describe()

filtered_df = df1.sort_values(by=['PROD_QTY'], ascending=False).head(10)
filtered_df

customer_purchases = df1[df1['LYLTY_CARD_NBR'] == 226000]
num_purchases = len(customer_purchases)
print(f"Customer 22600 made {num_purchases} purchases.")

df1 = df1[df1['LYLTY_CARD_NBR'] != 226000]

df1.describe()

transaction_counts = df1.groupby('DATE')['TXN_ID'].count()
print(transaction_counts)

# Create a sequence of dates
date_range = pd.date_range(start='2018-07-01', end='2019-06-30', freq='D')

# Create a DataFrame with the date range
all_dates_df = pd.DataFrame({'DATE': date_range})

# Merge with transaction_counts to fill in missing dates
transactions_by_day = pd.merge(all_dates_df, transaction_counts, on='DATE', how='left')

# Fill missing transaction counts with 0
transactions_by_day['TXN_ID'].fillna(0, inplace=True)

# Plot transactions over time
plt.figure(figsize=(12, 6))
plt.plot(transactions_by_day['DATE'], transactions_by_day['TXN_ID'])
plt.title('Transactions over time')
plt.xlabel('Day')
plt.ylabel('Number of transactions')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()


# Filter data for December
december_data = transactions_by_day[(transactions_by_day['DATE'] >= '2018-12-01') & (transactions_by_day['DATE'] <= '2018-12-31')]

# Plot transactions for December
plt.figure(figsize=(12, 6))
plt.plot(december_data['DATE'], december_data['TXN_ID'])
plt.title('Transactions in December')
plt.xlabel('Day')
plt.ylabel('Number of transactions')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Extract pack size using regular expressions
df1['PACK_SIZE'] = df1['PROD_NAME'].str.extract('(\d+)').astype(int)

# Check the distribution of pack sizes
print(df1['PACK_SIZE'].value_counts().sort_index())


plt.figure(figsize=(8, 6))  
plt.hist(df1['PACK_SIZE'], bins=20, edgecolor='black')  
plt.title('Distribution of Pack Sizes')
plt.xlabel('Pack Size')
plt.ylabel('Frequency')
plt.show()

df1['BRAND'] = df1['PROD_NAME'].str.split().str[0]

# Check the unique brands
print(df1['BRAND'].unique())

# Combine brand names
df1.loc[df1['BRAND'] == 'Snbts', 'BRAND'] = 'Sunbites'
df1.loc[df1['BRAND'] == 'Dorito', 'BRAND'] = 'Doritos'
df1.loc[df1['BRAND'] == 'Infzns', 'BRAND'] = 'Infuzions'
df1.loc[df1['BRAND'] == 'WW', 'BRAND'] = 'Woolworths'
df1.loc[df1['BRAND'] == 'Natural', 'BRAND'] = 'NCC'
df1.loc[df1['BRAND'] == 'Grain', 'BRAND'] = 'Grain Waves'

print(df1['BRAND'].unique())

df1.loc[df1['BRAND'] == 'GrnWves', 'BRAND'] = 'Grain Waves'

df1.loc[df1['BRAND'] == 'Red', 'BRAND'] = 'RRD'

df1.info()

df1.head()



df2 = pd.read_csv('/content/drive/MyDrive/Projects/Quantium Data Analytics Job Simulation/QVI_purchase_behaviour.csv')
df2.shape

df2.describe()

df2.describe(include='object')

df2.info()

df2.head(10)



merged_df = pd.merge(df1, df2, on='LYLTY_CARD_NBR', how='inner')

#changing object to category dtypes
dtype_changes = {
    'PROD_NAME': 'category',
    'BRAND': 'category',
    'LIFESTAGE': 'category',
    'PREMIUM_CUSTOMER': 'category'
}

merged_df = merged_df.astype(dtype_changes)

merged_df.isnull().sum()

dataset = merged_df.copy()

dataset.info()

'''from google.colab import files
dataset.to_csv('dataset.csv', encoding = 'utf-8-sig', index=False)
files.download('dataset.csv')'''

"""# Data Exploration
Questions:
1. What customer segments generate most revenue?
2. What products generate the most revenue?
3. What is the distribution of customers in each segment?
4. How many purchases were made by each customer segment?
5. How much do each customer segment spend averagely on chips?
"""

# PREMIUM_CUSTOMER vs TOT_SALES

# Calculate total sales for each premium customer segment
premium_customer_sales = merged_df.groupby('PREMIUM_CUSTOMER')['TOT_SALES'].sum().reset_index()
# Sort by total sales in descending order
premium_customer_sales = premium_customer_sales.sort_values(by=['TOT_SALES'], ascending=False)


plt.figure(figsize=(8, 6))
sns.barplot(x='PREMIUM_CUSTOMER', y='TOT_SALES', data=premium_customer_sales, order=premium_customer_sales['PREMIUM_CUSTOMER'])
plt.title('Total Sales by Premium Customer Segment')
plt.xlabel('Premium Customer Segment')
plt.ylabel('Total Sales')
plt.show()

"""Mainstream customers generate the most revenue ($700,000.00) among the premium customer segment

Premium customers generate the least revenue ($500,000.00) among the premium customer segment
"""

# LIFESTAGE vs TOT_SALES

# Calculate total sales for each lifestage
lifestage_sales = merged_df.groupby('LIFESTAGE')['TOT_SALES'].sum().reset_index()
# Sort by total sales in descending order
lifestage_sales = lifestage_sales.sort_values(by=['TOT_SALES'], ascending=False)

plt.figure(figsize=(12, 6)) 
sns.barplot(x='LIFESTAGE', y='TOT_SALES', data=lifestage_sales, order=lifestage_sales['LIFESTAGE'])
plt.title('Total Sales by Lifestage')
plt.xlabel('Lifestage')
plt.ylabel('Total Sales')
plt.xticks(rotation=45, ha='right') 
plt.show()

"""Older customers (singles/couples and families) and retirees generate the most revenue each about USD 350,000.00

New families generate the least revenue of just about USD 50,000.00

"""



# Group by LIFESTAGE and PREMIUM_CUSTOMER and sum TOT_SALES
segment_revenue = merged_df.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])['TOT_SALES'].sum()

# Sort by revenue in descending order
segment_revenue = segment_revenue.sort_values(ascending=False)
print(segment_revenue)

# Group by LIFESTAGE and PREMIUM_CUSTOMER and count unique customer IDs
customer_counts = merged_df.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])['LYLTY_CARD_NBR'].nunique()
print(customer_counts)

# Calculate total units and number of transactions for each segment
segment_data = merged_df.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])['PROD_QTY'].agg(['sum', 'count'])

# Rename columns for clarity
segment_data = segment_data.rename(columns={'sum': 'TOTAL_UNITS', 'count': 'TOTAL_TRANSACTIONS'})

# Calculate average units per transaction (treating each transaction as a customer)
segment_data['AVG_UNITS_PER_TRANSACTION'] = segment_data['TOTAL_UNITS'] / segment_data['TOTAL_TRANSACTIONS']
print(segment_data['AVG_UNITS_PER_TRANSACTION'])

# Calculate total units and unique customers for each segment
customer_units = merged_df.groupby(['LIFESTAGE', 'LYLTY_CARD_NBR'])['PROD_QTY'].agg(['sum', 'nunique'])

# Rename columns for clarity
customer_units = customer_units.rename(columns={'sum': 'TOTAL_UNITS', 'nunique': 'UNIQUE_CUSTOMERS'})

# Calculate average units per customer
customer_units['AVG_UNITS_PER_CUSTOMER'] = customer_units['TOTAL_UNITS'] / customer_units['UNIQUE_CUSTOMERS']
print(customer_units['AVG_UNITS_PER_CUSTOMER'])

# Create a box plot
plt.figure(figsize=(10, 6))
sns.boxplot(x='LIFESTAGE', y='AVG_UNITS_PER_CUSTOMER', data=customer_units.reset_index())
plt.title('Average Units per Customer by Lifestage')
plt.xlabel('Lifestage')
plt.ylabel('Average Units')
plt.xticks(rotation=45, ha='right') 
plt.show()

# Calculate total sales and total units for each segment
segment_prices = merged_df.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])[['TOT_SALES', 'PROD_QTY']].sum()

# Calculate average price per unit
segment_prices['AVG_PRICE_PER_UNIT'] = segment_prices['TOT_SALES'] / segment_prices['PROD_QTY']
print(segment_prices['AVG_PRICE_PER_UNIT'])

# Convert the multi-index to regular columns
segment_prices = segment_prices.reset_index()

# Create a grouped bar plot
plt.figure(figsize=(12, 6))
sns.barplot(x='LIFESTAGE', y='AVG_PRICE_PER_UNIT', hue='PREMIUM_CUSTOMER', data=segment_prices)
plt.title("Average Price per Unit by Customer Segment")
plt.xlabel("Lifestage")
plt.ylabel("Average Price per Unit")
plt.xticks(rotation=45, ha='right')
plt.legend(title="Premium Customer")
plt.show()

import scipy.stats as stats

# Extract data for Mainstream and Budget/Premium segments
'''mainstream_prices = segment_prices[segment_prices['PREMIUM_CUSTOMER'] == 'Mainstream']['AVG_PRICE_PER_UNIT']
budget_premium_prices = segment_prices[segment_prices['PREMIUM_CUSTOMER'].isin(['Budget', 'Premium'])]['AVG_PRICE_PER_UNIT']

# Perform independent t-test
t_statistic, p_value = stats.ttest_ind(mainstream_prices, budget_premium_prices)

# Print the results
print(f"T-statistic: {t_statistic:.2f}")
print(f"P-value: {p_value:.3f}")'''

import scipy.stats as stats

# Filter data for the two segments
segment1 = segment_prices[
    (segment_prices['PREMIUM_CUSTOMER'] == 'Mainstream') &
    (segment_prices['LIFESTAGE'].isin(['YOUNG SINGLES/COUPLES', 'MIDAGE SINGLES/COUPLES']))
]['AVG_PRICE_PER_UNIT']

segment2 = segment_prices[
    (segment_prices['PREMIUM_CUSTOMER'].isin(['Budget', 'Premium'])) &
    (segment_prices['LIFESTAGE'].isin(['YOUNG SINGLES/COUPLES', 'MIDAGE SINGLES/COUPLES']))
]['AVG_PRICE_PER_UNIT']

# Perform independent t-test
t_statistic, p_value = stats.ttest_ind(segment1, segment2)

# Print the results
print(f"T-statistic: {t_statistic:.2f}")
print(f"P-value: {p_value:.3f}")


# Filter for the target segment
mainstream_young_singles_couples = merged_df[
    (merged_df['PREMIUM_CUSTOMER'] == 'Mainstream') &
    (merged_df['LIFESTAGE'] == 'YOUNG SINGLES/COUPLES')
]

# Calculate top brands
top_brands = mainstream_young_singles_couples['BRAND'].value_counts().head(10)

# Visualize top brands
plt.figure(figsize=(10, 6))
sns.barplot(x=top_brands.index, y=top_brands.values)
plt.title('Top Brands Purchased by Mainstream Young Singles/Couples')
plt.xlabel('Brand')
plt.ylabel('Number of Purchases')
plt.xticks(rotation=45, ha='right')
plt.show()



# Filter for the target segment
mainstream_young_singles_couples = merged_df[
    (merged_df['PREMIUM_CUSTOMER'] == 'Mainstream') &
    (merged_df['LIFESTAGE'] == 'YOUNG SINGLES/COUPLES')
]

# Calculate top brands
top_pack_size = mainstream_young_singles_couples['PACK_SIZE'].value_counts().head(10)

# Visualize top brands
plt.figure(figsize=(10, 6))
sns.barplot(x=top_pack_size.index, y=top_pack_size.values)
plt.title('Top Pack Sizes Purchased by Mainstream Young Singles/Couples')
plt.xlabel('Pack Size')
plt.ylabel('Number of Purchases')
plt.xticks(rotation=45, ha='right')
plt.show()

merged_df.info()

import seaborn as sns
import matplotlib.pyplot as plt

# Group data by customer segments and get pack sizes
# pack_sizes_by_segment = merged_df.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])['PACK_SIZE'].apply(list).reset_index()
pack_sizes_by_segment = merged_df.explode('PACK_SIZE')

# Create a box plot
plt.figure(figsize=(12, 8))
sns.boxplot(x='LIFESTAGE', y='PACK_SIZE', hue='PREMIUM_CUSTOMER', data=pack_sizes_by_segment)
plt.title('Preferred Pack Sizes Across Customer Segments')
plt.xlabel('Lifestage')
plt.ylabel('Pack Size')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Premium Customer')
plt.show()

# Calculate average spending per purchase (transaction)
average_spending = mainstream_young_singles_couples.groupby('TXN_ID')['TOT_SALES'].sum().mean()
print(f"Average Spending per Purchase: {average_spending:.2f}")

#Histogram of average spending of mainstream, young singles/couples
plt.figure(figsize=(8, 6))
plt.hist(mainstream_young_singles_couples.groupby('TXN_ID')['TOT_SALES'].sum(), bins=20, edgecolor='black')
plt.title('Distribution of Spending per Purchase (Mainstream, Young Singles/Couples)')
plt.xlabel('Spending per Purchase')
plt.ylabel('Frequency')
plt.show()

# Transforming data into a format suitable for a-priori analysis analysis

# Filter for the target segment
target_segment = merged_df[
    (merged_df['PREMIUM_CUSTOMER'] == 'Mainstream') &
    (merged_df['LIFESTAGE'] == 'YOUNG SINGLES/COUPLES')
]

# Create a list of transactions with product names
transactions = target_segment.groupby('TXN_ID')['PROD_NAME'].apply(list).tolist()

# Create a binary matrix
from mlxtend.preprocessing import TransactionEncoder

te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
transaction_matrix = pd.DataFrame(te_ary, columns=te.columns_)

from mlxtend.frequent_patterns import apriori, association_rules

# Find frequent itemsets with minimum support of 0.005
frequent_itemsets = apriori(transaction_matrix, min_support=0.005, use_colnames=True)

# Generate association rules with minimum confidence of 0.5
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)

import seaborn as sns
import matplotlib.pyplot as plt

# Create a pivot table for the heatmap
pivot_table = rules.pivot(index='antecedents', columns='consequents', values='confidence')

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(pivot_table, annot=True, fmt=".2f", cmap="YlGnBu")
plt.title("Product Associations (Confidence)")
plt.xlabel("Consequents")
plt.ylabel("Antecedents")
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.show()

#!pip install apyori
from apyori import apriori

# Apply apriori algorithm
results = list(apriori(transactions, min_support=0.01, min_confidence=0.5))

# Filter and display results
'''for item in results:
    # Extract antecedent and consequent
    antecedent = list(item[2][0][0])[0]
    consequent = list(item[2][0][1])[0]

    # Display support and confidence
    print(f"Rule: {antecedent} -> {consequent}")
    print(f"Support: {item[1]:.3f}")
    print(f"Confidence: {item[2][0][2]:.3f}")
    print("-" * 20)'''

for item in results:
    # Check if the rule has an antecedent and consequent
    if len(item[2]) > 0 and len(item[2][0]) > 1:  # Check if rule and its parts exist
        antecedent = list(item[2][0][0])[0]  # Access antecedent safely
        consequent = list(item[2][0][1])[0]  # Access consequent safely

        # Display support and confidence
        print(f"Rule: {antecedent} -> {consequent}")
        print(f"Support: {item[1]:.3f}")
        print(f"Confidence: {item[2][0][2]:.3f}")
        print("-" * 20)
    else:
        print("Rule skipped due to missing antecedent or consequent.")


# Group by LIFESTAGE and PREMIUM_CUSTOMER and count unique TXN_ID (number of customers)
segment_customers = merged_df.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])['TXN_ID'].nunique()

# Sort by number of customers in descending order
segment_customers = segment_customers.sort_values(ascending=False)
print(segment_customers)











